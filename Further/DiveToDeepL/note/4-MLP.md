# 多层感知机

## 1 引入隐藏层

**线性模型可能出错**

- 线性意味着<u>单调假设</u>：任何特征的增⼤都会导致模型输出的增⼤（如果对应的权重为正）
- 任何像素的重要性都以复杂的⽅式取决于该像素的上下⽂（周围像素的值）

**在网络中加入隐藏层**

- 加入一个或多个隐藏层以克服模型限制

	- 最简单：将许多全链接层堆叠在一起

	<img src="https://raw.githubusercontent.com/Masshiro/TyporaImages/master/20230715203142.png" style="zoom:80%;" />

**从线性到非线性**

- 符号：
  - $\mathbf{X}\in \mathbb{R}^{n\times d}$: $n$个样本的小批量，每个样本$d$个输入特征
  - $\mathbf{H}\in\mathbb{R}^{n\times h}$：隐藏层输出，称为隐藏表示(hidden representations)
    - $\mathbf{W}^{(1)}\in\mathbb{R}^{d\times h}$：隐藏层权重
    - $\mathbf{b}^{(1)}\in\mathbb{R}^{1\times h}$：隐藏层偏置
  - $\mathbf{O}\in\mathbb{R}^{n\times q}$：输出层输出
    - $\mathbf{W}^{(2)}\in\mathbb{R}^{h\times q}$：输出层权重
    - $\mathbf{b}^{(2)}\in\mathbb{R}^{1\times q}$：输出层偏置

- 合并隐藏层，得到等价的单层模型：
  $$
  \begin{aligned}
  \mathbf{O} &=(\mathbf{XW}^{(1)}+\mathbf{b}^{(1)} )\mathbf{W}^{(2)}+\mathbf{b}^{(2)}\\
  &=\mathbf{XW}+\mathbf{b}
  \end{aligned}
  $$

  - $\mathbf{W}=\mathbf{W}^{(1)}\mathbf{W}^{(2)}$, $\mathbf{b}=\mathbf{b}^{(1)}\mathbf{W}^{(2)}+\mathbf{b}^{(2)}$

- 对每个隐藏单元应用非线性的激活函数以发挥多层架构的潜力：

  - 有了激活函数，多层感知机不会退化为线性模型：
    $$
    \begin{aligned}
    \mathbf{H} &= \sigma(\mathbf{XW}^{(1)}+\mathbf{b}^{(1)})\\
    \mathbf{O} &= \mathbf{HW}^{(2)}+\mathbf{b}^{(2)}
    \end{aligned}
    $$

## 2 激活函数

- 通过计算加权和并加上偏置来确定神经元是否应该被激活

**ReLU**

- 实现简单，对给定元素$x$，ReLU函数被定义为该元素与$0$的最大元素
  $$
  \text{ReLU}=\max(x,0)
  $$

- 有很多变体：

  - 参数化ReLU
    $$
    p\text{ReLU}(x)=\max(x,0)+\alpha \min(0,x)
    $$

**sigmoid**

- 通常称为压缩函数：将范围$(-\infty,\infty)$中任意输出压缩至$(0,1)$

$$
\text{sigmoid}(x)=\frac{1}{1+\exp(-x)}
$$

- sigmoid在隐藏层中已经较少使⽤，它在⼤部分时候被更简单、 更容易训练的ReLU所取代

**tanh**

- tanh(双曲正切)函数也能将其输⼊压缩转换到区间$(-1,1)$上
  $$
  \tanh(x)=\frac{1-\exp(-2x)}{1+\exp(-2x)}
  $$

## 3 模型选择、欠/过拟合

- 目标是发现模式（pattern）：
	- 希望模式捕捉到训练集潜在的总体规律
	- 发现可以泛化的模式
- 一些概念：
	- 过拟合：模型在训练数据上拟合的比在潜在分布中更接近的现象
	- ==<u>***正则化：用于对抗过拟合的技术***</u>==

**训练误差&泛化误差**

- 基本定义
	- 训练误差：模型在训练数据集上计算得到的误差
	- 泛化误差：模型应用在同样从原始样本的分布中抽取的无限多数据样本时，模型误差的期望
- 无法准确计算出泛化误差

**倾向于影响模型泛化的因素**

- 可调参数的数量：参数很大时（有时称为自由度），模型通常更容易过拟合
- 参数采用的值：权重取值范围较大时，模型容易过拟合
- 训练样本的数量：即使模型很简单，也很容易过拟合只包含⼀两个样本的数据集。⽽过拟合⼀个有数百 万个样本的数据集则需要⼀个极其灵活的模型。

### 3.1 模型选择

- 机器学习中，通常评估几个候选模型后选择最终的模型
	- 本质不同的模型（决策树与线性模型）
	- 对比不同超参数设置下的同类模型
- 为确定最佳模型，需要验证集

**欠拟合或过拟合**

- 比较训练和验证误差时，两种常见情况：
	- 训练误差和验证误差都很严重，但仅有一点差距
		- 模型不能降低训练误差：模型过于简单（表达能力不足）
		- 欠拟合，可用更复杂的模型训练误差
	- 训练误差明显低于验证误差
		- 过拟合的表现
		- 并不总是坏事：最好的预测模型在训练数据上的表现往往⽐在保留（验证）数据上好得多
		- 最终，通常<u>更关⼼验证误差</u>，⽽不是训练误差和验证误差之间的差距

## 4 权重衰减

- 总是可以通过收集更多的训练数据来缓解过拟合

- 在训练参数化机器学习模型时，*<u>权重衰减（weight decay）是最⼴泛使⽤的正则化的技术之⼀</u>*，
	- 通常也被 称为$L_2$正则化
	- 通过函数与0的距离来衡量函数的复杂度($f=0$在某种意义上是最简单的)
	
- 一种简单的方法是通过线性函数$f(\mathbf{x}=\mathbf{w}^\top\mathbf{x})$中的权重向量的某个范数来度量其复杂性
	- $\Vert \mathbf{w}\Vert^2$
	- 通常将其范数<u>作为惩罚项</u>加到最⼩化损失的问题中
	
- 损失函数与用来验证数据拟合的公式
	
	- $\lambda$：非负超参数
	  $$
	  L(\mathbf{w},b)=\frac{1}{n}\sum_{i=1}^{n}\frac{1}{2}(\mathbf{w}^\top\mathbf{x}^{(i)}+b-y^{(i)})^2\\
	  L(\mathbf{w},b)+\frac{\lambda}{2}\Vert\mathbf{w}\Vert^2
	  $$
	
- 

- 希望模型深度挖掘特征，即将其权重分散到许多特征中，而非过于依赖少数潜在的虚假关联

## 5 暂退法（Dropout）

- 线性模型没有考虑到特征之间的交互作用：

	- 对于每个特征，线性模型必须指定正或负的权重，而忽略其他特征

- "<u>*简单性*</u>"的另一角度：平滑性

	- 预计向像素添加一些随机噪声应该基本无影响

- Dropout：

	- 前向传播过程中，计算每一内部层的同时注入噪声
	- 表面上看是在训练过程中丢弃一些神经元
	- 在整个训练过程的每⼀次迭代中，标准暂退法包括在计算下⼀层之前将当前层中的⼀些节点置零

- 注入噪声的方法：

	- unbiased：在固定住其他层时，每⼀层的期望值等于没有噪⾳时的值。

	- 标准暂退法正则化中，通过按保留（未丢弃）的节点的分数进⾏规范化来消除每⼀层的偏差。

		- 每个中间活性值$h$以暂退概率$p$由随机变量$h'$替换
			$$
			h'=\begin{cases}
			0, & \text{概率为$p$}\\
			\frac{h}{1-p}, & \text{其他情况}
			\end{cases}
			$$

		- 该模型期望不变：$E[h']=h$

- 实践中的暂退法

	<img src="https://raw.githubusercontent.com/Masshiro/TyporaImages/master/20230717140354.png" style="zoom:80%;" />

	- 计算输出不再依赖$h_2$或$h_5$，其各自的梯度在执行反向传播时也会消失

## 6 前向传播

- 指：按顺序（从输入层到输出层）计算和存储神经网络中每层的结果

- 假设输入样本是$\mathbf{x}\in\mathbb{R}^d$，且隐藏层不包含偏置项，则：

	- 长度为$h$的隐藏激活向量，其中$\mathbf{W}^{(1)}\in\mathbb{R}^{h\times d}$：
		$$
		\mathbf{h}=\phi(\mathbf{z})=\phi(\mathbf{W}^{(1)}\mathbf{x})
		$$

	- 设输出层参数只有权重$\mathbf{W}^{(2)}\in\mathbb{R}^{q\times h}$，则输出层变量为一个长度为$q$的向量：
		$$
		\mathbf{o}=\mathbf{W}^{(2)}\mathbf{h}
		$$

	- 设损失函数为$l$，样本标签为$y$，则计算单个数据样本的损失项：
		$$
		L=l(\mathbf{o},y)
		$$

	- 根据$L_2$正则化定义，给定超参数$\lambda$，正则化项：
		$$
		s=\frac{\lambda}{2}\left(\Vert\mathbf{W}^{(1)}\Vert^2_F+\Vert\mathbf{W}^{(2)}\Vert^2_F \right)
		$$

	- 模型在给定数据样本上的正则化损失为：
		$$
		J=L+s
		$$

- 前向传播计算图

	<img src="https://raw.githubusercontent.com/Masshiro/TyporaImages/master/20230717201833.png" style="zoom:80%;" />

## 7 反向传播

- 指：计算神经网络参数梯度的方法
- 根据微积分中的链式规则，按相反的顺序从输出层到输入层遍历网络
- 该算法存储了计算某些参数梯度时，所需的所有中间变量（偏导数）
