# 1 CNN的引入

- 目前为止，对于图像这类结构丰富的数据，处理方式不够有效
	- 仅将图像数据展平成一维向量而忽略每个图像的空间结构
- 由于网络特征元素的顺序不变，因此最优的结果是利用先验知识（相近像素间的相互关联性）进行学习
- CNN：为处理图像数据而设计
- 所有CNN主干的基本元素：
	- 卷积层本身
	- 填充（padding）和步幅（stride）的基本细节
	- 用于在相邻区域汇聚信息的汇聚层（pooling）
	- 在每一层中多通道（channel）

# 2 全链接层到卷积

- 多层感知机：适合处理表格数据
	- 行：样本；列：特征
	- 表格数据：寻找的模式可能涉及特征之间的交互，但是我们不能预先假设任何与特征交互相关的先验结构

## 2.1 不变性

- 从图中找到某个物体时的合理假设：⽆论哪种⽅法找到这个物体，都应该和物体的位置⽆关
- CNN将空间不变性（spatial invariance）系统化：
	- **<u>平移不变性（translation invariance）</u>**：不管检测对象出现在图像中的哪个位置，神经⽹络的前⾯⼏层应该对相同的图像区域具有相似的反应，即为“平移不变性”。
	- **<u>局部性（locality）</u>**：神经⽹络的前⾯⼏层应该只探索输⼊图像中的局部区域，⽽不过度在意图像中相隔较远区域的关系，这就是“局部性”原则。最终，可以聚合这些局部特征，以在整个图像级别进⾏预测。

## 2.2 MLP的限制

- 多层感知机：

	- 输入：二维图像$\mathbf{X}$
	- 隐藏表示：$\mathbf{H}$：数学上为矩阵，代码上为二维张量
	- $\mathbf{X,H}$具有相同形状
	- $[\mathbf{X}]_{i,j},[\mathbf{H}]_{i,j}$：输入图像和隐藏表示中位置$(i,j)$处的像素
	- 参数：权重矩阵 $\Rightarrow$ 四阶权重张量`W`
		- 目的：使每个隐藏神经元都可接收到每个输入像素的信息
		- [张量的阶数是指张量可以表示为多少个向量的张量](https://www.zhihu.com/question/380486882)

- 全链接层形式化表示：
	$$
	\begin{aligned}
	\mathbf{[H]}_{i,j} &= [\mathbf{U}]_{i,j} + \sum_k\sum_l[\texttt{W}]_{i,j,k,l}[\mathbf{X}]_{k,l}\\
	&=[\mathbf{U}]_{i,j} + \sum_a\sum_b[\texttt{V}]_{i,j,a,b}[\mathbf{X}]_{i+a,j+b}
	\end{aligned}
	$$

	- 平移不变性

		- 检测对象在输入$\mathbf{X}$中平移，应仅导致$\mathbf{H}$中的平移 

		- $\texttt{V}$和$\mathbf{U}$不依赖于$(i,j)$的值，即$[\texttt{V}]_{i,j,a,b}=[\mathbf{V}]_{a,b}$，且$\mathbf{U}$是一个常数，如$u$

		- 可简化$\mathbf{H}$：
			$$
			[\mathbf{H}]_{i,j}=u+\sum_a\sum_b[\mathbf{V}]_{a,b}[\mathbf{X}]_{i+a,j+b}
			$$

			- 此即为卷积
			- 使用系数$[\mathbf{V}]_{a,b}$对位置$(i,j)$附近的像素$(i+a,j+b)$进行加权得到$[\mathbf{H}]_{i,j}$

		- $[\mathbf{V}]_{a,b}$系数比$[\texttt{V}]_{i,j,a,b}$少很多，因为前者不再依赖图像中的位置

	- 局部性

		- 对于$\vert a\vert>\Delta$或$\vert b\vert>\Delta$的范围之外，设置$[\mathbf{V}]_{a,b}=0$

		- 重写式子：
			$$
			[\mathbf{H}]_{i,j}=u+\sum_{a=-\Delta}^\Delta\sum_{b=-\Delta}^\Delta[\mathbf{V}]_{a,b}[\mathbf{X}]_{i+a,j+b}
			$$

			- 此即为一个卷积层
			- $\mathbf{V}$：卷积核/滤波器，或简称卷积层的权重，通常为可学习的参数

- 缺陷：忽略了图像一般包含三个通道（三原色）：

	- 图像并非二维张量，而是<u>*⼀个由⾼度、宽度和颜⾊组成的三维张量*</u>
	- 前两个轴与像素的空间位置有关，⽽第三个轴可以看作每个像素的多维表示
	- 可以把隐藏表示想象为⼀系列具有⼆维张量的通道（channel）



# 3 图像卷积

## 3.1 互相关运算

![](https://raw.githubusercontent.com/Masshiro/TyporaImages/master/20230719165325.png)

- 计算规则：$0\times 0+1\times 1+3\times2+4\times3=19$

**输入输出大小关系：**

- 输出大小略小于输入大小
	- 因为卷积核的宽度和⾼度⼤于1
	- 卷积核只与图像中每个⼤⼩完全适合的位置进⾏互相关运算
- 输入大小$n_h\times n_w$，卷积核大小$k_h\times k_w$，则输出大小为$(n_h-k_h+1)\times(n_w-k_w+1)$

## 3.2 填充&步幅

- 两种应用场景：
	- 填充：避免图像在多次卷积后，图像边界丢失有用信息
		- $240\times 240$像素的图像，10层$5\times5$卷积后，将减少至$200\times200$
	- 步幅：原始输入分辨率过于冗余时

- **填充**：在输入图像的边界填充元素（通常为0）

	![](https://raw.githubusercontent.com/Masshiro/TyporaImages/master/20230719170427.png)

	- 行填充和列填充的行数分别为：$p_h, p_w$
	- 为使输入输出具有相同高度和宽度，设置$p_h=k_h-1$及$p_w=k_w-1$
	- 一般设置卷积核高宽为奇数（如1、3或7）：
		- 保持空间维度
		- 在顶部底部，以及在左侧右侧填充相同数量的行与列（$p_h/2$与$p_w/2$）

- **步幅**：分为水平步幅与垂直步幅

	![](https://raw.githubusercontent.com/Masshiro/TyporaImages/master/20230719172315.png)

	- 为了计算输出中第⼀列的第⼆个元素和第⼀⾏的第⼆个元素，卷积窗⼝分别向下滑动三⾏和向右滑动两列
	- 当卷积窗⼝继续向右滑动两列时，没有输出，因为输⼊元素⽆法填充窗⼝（除⾮我们添加另⼀列填充）

## 4 多输入多输出通道

- 添加通道时，输入和隐藏的表示均为三维张量
	- 如：每个RGB输入图像具有$3\times h\times w$的形状
	- 将该大小为$3$的轴称为通道维度

## 4.1 多输入通道

- 需要构造一个和输入通道数相同的卷积核

	- 均为$c_i$

- 若卷积核的窗口形状为$k_h\times k_w$，则当$c_i=1$时，可将卷积核视为$k_h\times k_w$的二维张量

- 以双通道为例：

	![](https://raw.githubusercontent.com/Masshiro/TyporaImages/master/20230720160921.png)

## 4.2 多输出通道

- 直观讲：可将每个通道看作对不同特征的响应
- 现实更复杂：每个通道并非独立学习，而是为了共同使用而优化
- 符号表示：
	- $c_i$，$c_o$：输入和输出通道的数目
	- 为每个输出通道创建$c_i\times k_h\times k_w$的卷积核张量
	- 卷积核形状：$c_o\times c_i\times k_h\times k_w$
- ==计算规则==：每个输出通道先获取所有输入通道，再以对应该输出通道的卷积核进行计算

## 4.3 $1\times 1$卷积核

- 缺点：失去了卷积层的特有能⼒——在⾼度和宽度维度上，识别相邻元素间相互作⽤的能⼒

	![](https://raw.githubusercontent.com/Masshiro/TyporaImages/master/20230720163229.png)

	- 可将$1\times 1$卷积层视为在每个像素位置应用的全链接层，以$c_i$个输入值转换为$c_o$个输出值

# 4 汇聚层

- 最后一层的神经元应对整个输入的全局敏感

- 通过逐渐聚合信息，生成越来越粗糙的映射，最终实现全局表示的目标

- 汇聚层目的：

	- 降低卷积层对位置的敏感性
	- 降低对空间降采样表示的敏感性

	![](https://raw.githubusercontent.com/Masshiro/TyporaImages/master/20230720165451.png)

	