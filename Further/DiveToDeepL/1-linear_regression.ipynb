{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 线性回归从零实现\n",
    "- 了解细致的工作原理：\n",
    "  - 方便自定义模型、自定义层或自定义损失函数\n",
    "- 只使用张量&自动求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import random \n",
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 生成数据集\n",
    "- 使用低维数据\n",
    "- 带有噪声\n",
    "- 生成1000个样本：\n",
    "  - 每个样本包含从正态分布中采样的2个特征\n",
    "  - 合成数据集为一个矩阵$\\mathbf{X}\\in\\mathbb{R}^{1000\\times2}$\n",
    "\n",
    "- 使用线性参数$\\mathbf{w}=[2,-3.4]^\\top$、$b=4.2$以及$\\epsilon$生成数据集和标签\n",
    "  - $\\epsilon$服从均值为0的正态分布\n",
    "  - 将标准差设为0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: tensor([-0.7786,  0.3009]) \n",
      "label: tensor([1.6215])\n"
     ]
    }
   ],
   "source": [
    "true_w = torch.tensor([2, -3.4])\n",
    "true_b = 4.2\n",
    "def synthetic_data(w, b, num_examples):\n",
    "    X = torch.normal(0, 1, (num_examples, len(w)))\n",
    "    y = torch.matmul(X, w) + b\n",
    "    y += torch.normal(0, 0.01, y.shape)\n",
    "    return X, y.reshape((-1, 1))\n",
    "\n",
    "features, labels = synthetic_data(true_w, true_b, 1000)\n",
    "print('features:', features[0],'\\nlabel:', labels[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 读取数据集\n",
    "- 训练模型时需要对数据集进行遍历，每次抽取一小批量样本，并使用其更新模型\n",
    "- 需要定义一个函数：\n",
    "  - 可以打乱数据集中的样本\n",
    "  - 并以小批量方式获取数据\n",
    "- 定义该函数为`data_iter`\n",
    "  - 接收参数：批大小、特征矩阵和标签向量\n",
    "  - 输出：大小为`batch_size`的小批量\n",
    "- `yeild`关键字\n",
    "  - `yield`与`return`的区别是，`return`会返回一个最终的值，并结束函数的执行，而`yield`可以返回多个值，并保持函数的状态\n",
    "  - 可以用来在一个函数中返回一个生成器对象。生成器是一种特殊的函数，它不会一次性返回所有的值，而是每次返回一个值，然后暂停执行，直到下一次请求。\n",
    "  - 当一个带有yield的函数被调用时，执行会停在`yield`语句处，并在生成器被迭代时从那里继续\n",
    "- 注意：\n",
    "  - 下述函数在实际应用中**执行效率很低**\n",
    "  - 要求所有数据加载到内存并且执行大量随机内存访问"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    random.shuffle(indices)  # Shuffle list in place, and return None.\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        batch_indices = torch.tensor (\n",
    "            indices[i: min(i + batch_size, num_examples)])\n",
    "        yield features[batch_indices], labels[batch_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4672, -2.2855],\n",
      "        [-0.7969,  0.4304],\n",
      "        [-0.9522, -0.4036],\n",
      "        [-0.1783, -0.1045],\n",
      "        [ 0.9622, -2.8179],\n",
      "        [-0.3835, -0.5939],\n",
      "        [-0.1761,  1.6146],\n",
      "        [ 0.6210, -0.6852],\n",
      "        [-0.5481, -0.9690],\n",
      "        [-1.4299, -1.3684]]) \n",
      " tensor([[12.8972],\n",
      "        [ 1.1432],\n",
      "        [ 3.6466],\n",
      "        [ 4.1974],\n",
      "        [15.7206],\n",
      "        [ 5.4521],\n",
      "        [-1.6411],\n",
      "        [ 7.7589],\n",
      "        [ 6.3909],\n",
      "        [ 6.0013]])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "\n",
    "for X, y in data_iter(batch_size, features, labels):\n",
    "    print(X, '\\n', y) \n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 初始化模型参数\n",
    "- 在使用小批量随机梯度下降优化模型参数前，需要先有一些参数\n",
    "  - 通过从均值为0、标准差为0.01的正态分布中采样随机数来初始化权重，并将偏置初始化为0\n",
    "- 初始化参数后，更新它们，使得其足以拟合数据\n",
    "  - 每次更新都要计算损失函数关于模型参数的梯度\n",
    "  - 有该梯度，则可向减少损失的方向更新每个参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized parameters:\n",
      " \tw =  tensor([[-0.0035],\n",
      "        [-0.0077]], requires_grad=True) \n",
      "\tb =  tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "w = torch.normal(0, 0.01, size=(2,1), requires_grad=True) \n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "print(\"Initialized parameters:\\n\", \"\\tw = \", w, \"\\n\\tb = \", b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 定义模型及损失函数\n",
    "- 定义模型：\n",
    "  - 将模型的输入和参数同模型的输出关联起来\n",
    "  - 计算输出：输入特征$\\mathbf{X}$和模型权重$\\mathbf{w}$的矩阵-向量乘法后再加上一个偏置$b$\n",
    "- 定义损失函数：\n",
    "  - 使用平方损失函数\n",
    "  - 实现时，需要将真实值`y`的形状转换为和预测值`y_hat`的形状相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg(X, w, b):\n",
    "    return torch.matmul(X, w) + b\n",
    "\n",
    "def squared_loss(y_hat, y):\n",
    "    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 定义优化算法\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
