{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 线性回归从零实现\n",
    "- 了解细致的工作原理：\n",
    "  - 方便自定义模型、自定义层或自定义损失函数\n",
    "- 只使用张量&自动求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import random \n",
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 生成数据集\n",
    "- 使用低维数据\n",
    "- 带有噪声\n",
    "- 生成1000个样本：\n",
    "  - 每个样本包含从正态分布中采样的2个特征\n",
    "  - 合成数据集为一个矩阵$\\mathbf{X}\\in\\mathbb{R}^{1000\\times2}$\n",
    "\n",
    "- 使用线性参数$\\mathbf{w}=[2,-3.4]^\\top$、$b=4.2$以及$\\epsilon$生成数据集和标签\n",
    "  - $\\epsilon$服从均值为0的正态分布\n",
    "  - 将标准差设为0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: tensor([-0.0127,  1.1306]) \n",
      "label: tensor([0.3412])\n"
     ]
    }
   ],
   "source": [
    "true_w = torch.tensor([2, -3.4])\n",
    "true_b = 4.2\n",
    "def synthetic_data(w, b, num_examples):\n",
    "    X = torch.normal(0, 1, (num_examples, len(w)))\n",
    "    y = torch.matmul(X, w) + b\n",
    "    y += torch.normal(0, 0.01, y.shape)\n",
    "    return X, y.reshape((-1, 1))\n",
    "\n",
    "features, labels = synthetic_data(true_w, true_b, 1000)\n",
    "print('features:', features[0],'\\nlabel:', labels[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 读取数据集\n",
    "- 训练模型时需要对数据集进行遍历，每次抽取一小批量样本，并使用其更新模型\n",
    "- 需要定义一个函数：\n",
    "  - 可以打乱数据集中的样本\n",
    "  - 并以小批量方式获取数据\n",
    "- 定义该函数为`data_iter`\n",
    "  - 接收参数：批大小、特征矩阵和标签向量\n",
    "  - 输出：大小为`batch_size`的小批量\n",
    "- `yeild`关键字\n",
    "  - `yield`与`return`的区别是，`return`会返回一个最终的值，并结束函数的执行，而`yield`可以返回多个值，并保持函数的状态\n",
    "  - 可以用来在一个函数中返回一个生成器对象。生成器是一种特殊的函数，它不会一次性返回所有的值，而是每次返回一个值，然后暂停执行，直到下一次请求。\n",
    "  - 当一个带有yield的函数被调用时，执行会停在`yield`语句处，并在生成器被迭代时从那里继续\n",
    "- 注意：\n",
    "  - 下述函数在实际应用中**执行效率很低**\n",
    "  - 要求所有数据加载到内存并且执行大量随机内存访问"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    random.shuffle(indices)  # Shuffle list in place, and return None.\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        batch_indices = torch.tensor (\n",
    "            indices[i: min(i + batch_size, num_examples)])\n",
    "        yield features[batch_indices], labels[batch_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5623, -0.6196],\n",
      "        [ 1.7025, -1.1421],\n",
      "        [ 1.2690, -0.1246],\n",
      "        [-2.1959, -0.9279],\n",
      "        [ 1.4713,  0.6683],\n",
      "        [ 0.5869,  1.0196],\n",
      "        [-1.5914,  0.7038],\n",
      "        [-0.4241, -0.0414],\n",
      "        [-0.2838,  0.1662],\n",
      "        [-0.5193,  1.5036]]) \n",
      " tensor([[ 5.1717],\n",
      "        [11.4954],\n",
      "        [ 7.1652],\n",
      "        [ 2.9512],\n",
      "        [ 4.8834],\n",
      "        [ 1.9014],\n",
      "        [-1.3945],\n",
      "        [ 3.4953],\n",
      "        [ 3.0577],\n",
      "        [-1.9591]])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "\n",
    "for X, y in data_iter(batch_size, features, labels):\n",
    "    print(X, '\\n', y) \n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 初始化模型参数\n",
    "- 在使用小批量随机梯度下降优化模型参数前，需要先有一些参数\n",
    "  - 通过从均值为0、标准差为0.01的正态分布中采样随机数来初始化权重，并将偏置初始化为0\n",
    "- 初始化参数后，更新它们，使得其足以拟合数据\n",
    "  - 每次更新都要计算损失函数关于模型参数的梯度\n",
    "  - 有该梯度，则可向减少损失的方向更新每个参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized parameters:\n",
      " \tw =  tensor([[ 0.0039],\n",
      "        [-0.0015]], requires_grad=True) \n",
      "\tb =  tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "w = torch.normal(0, 0.01, size=(2,1), requires_grad=True) \n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "print(\"Initialized parameters:\\n\", \"\\tw = \", w, \"\\n\\tb = \", b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 定义模型及损失函数\n",
    "- 定义模型：\n",
    "  - 将模型的输入和参数同模型的输出关联起来\n",
    "  - 计算输出：输入特征$\\mathbf{X}$和模型权重$\\mathbf{w}$的矩阵-向量乘法后再加上一个偏置$b$\n",
    "- 定义损失函数：\n",
    "  - 使用平方损失函数\n",
    "  - 实现时，需要将真实值`y`的形状转换为和预测值`y_hat`的形状相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg(X, w, b):\n",
    "    return torch.matmul(X, w) + b\n",
    "\n",
    "def squared_loss(y_hat, y):\n",
    "    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 定义优化算法\n",
    "- 定义函数实现小批量随机梯度下降更新\n",
    "  - 从数据集中随机抽取⼀个⼩批量，然后根据参数计算损失的梯度\n",
    "  - 朝着减少损失的⽅向更新参数\n",
    "- 输入：模型参数集合、学习速率（`lr`）和批大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params, lr, batch_size):\n",
    "    with torch.no_grad():\n",
    "        for param in params:\n",
    "            param -= lr * param.grad / batch_size\n",
    "            param.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 训练\n",
    "- 需要**重点理解**这部分代码，日后反复用到\n",
    "- 每次迭代中，读取小批量训练样本，并通过模型来获得一组预测\n",
    "- 计算完损失后，开始反向传播，存储每个参数的梯度\n",
    "- 最后调用优化算法`sgd`来更新参数\n",
    "- 概括而言，执行以下循环：\n",
    "  - 初始化参数\n",
    "  - 重复训练，直至完成：\n",
    "    - 计算梯度：$\\mathbf{g}\\leftarrow \\partial_{(\\mathbf{w},b)}\\frac{1}{\\vert \\mathcal{B}\\vert}\\sum_{i\\in \\mathcal{B}} l(\\mathbf{x}^{(i)}, y^{(i)}, \\mathbf{w}, b)$ \n",
    "    - 更新参数：$(\\mathbf{w},b)\\leftarrow (\\mathbf{w},b)-\\eta \\mathbf{g}$\n",
    "- 每个epoch，使用`data_iter`函数便利整个数据集，并将训练数据集中所有样本都使用一次\n",
    "- 超参数：\n",
    "  - `num_epochs=3`：迭代周期数\n",
    "  - `lr=0.03`：学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.000049\n",
      "epoch 2, loss 0.000049\n",
      "epoch 3, loss 0.000049\n",
      "w的估计误差: tensor([-7.3910e-06, -1.3399e-04], grad_fn=<SubBackward0>)\n",
      "b的估计误差: tensor([-0.0005], grad_fn=<RsubBackward1>)\n"
     ]
    }
   ],
   "source": [
    "lr = 0.03\n",
    "num_epochs = 3\n",
    "net = linreg\n",
    "loss = squared_loss\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter(batch_size, features, labels):\n",
    "        l = loss(net(X, w, b), y) # X和y的小批量损失\n",
    "        # 因为l形状是(batch_size,1)，⽽不是⼀个标量。l中的所有元素被加到⼀起，\n",
    "        # 并以此计算关于[w,b]的梯度\n",
    "        l.sum().backward()\n",
    "        sgd([w, b], lr, batch_size) # 使⽤参数的梯度更新参数\n",
    "    with torch.no_grad():\n",
    "        train_l = loss(net(features, w, b), labels)\n",
    "        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')\n",
    "\n",
    "print(f'w的估计误差: {true_w - w.reshape(true_w.shape)}')\n",
    "print(f'b的估计误差: {true_b - b}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 线性回归简洁实现\n",
    "## 2.1 生成并读取数据集\n",
    "- 读取数据集时:\n",
    "  - 调用框架中现有的API读取\n",
    "  - 将`features`和`labels`作为API的参数进行传递\n",
    "  - `is_train`表示是否希望数据迭代器对象在每次迭代周期内打乱数据\n",
    "- 验证`data_iter`函数是否工作正常：\n",
    "  - `iter`构造Python迭代器\n",
    "  - `next`从迭代器中获取第一项\n",
    "- https://pytorch.org/docs/stable/data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0.4379,  0.0283],\n",
       "         [-0.0777,  1.5518],\n",
       "         [ 0.1473, -1.1050],\n",
       "         [ 1.5232, -1.4536],\n",
       "         [-1.2868, -0.9253],\n",
       "         [-0.0658, -0.0709],\n",
       "         [-0.2769,  0.2918],\n",
       "         [-1.1718, -0.6670],\n",
       "         [-0.3410,  1.9344],\n",
       "         [-0.4746,  0.8176]]),\n",
       " tensor([[ 4.9633],\n",
       "         [-1.2268],\n",
       "         [ 8.2603],\n",
       "         [12.1800],\n",
       "         [ 4.7684],\n",
       "         [ 4.3085],\n",
       "         [ 2.6473],\n",
       "         [ 4.1303],\n",
       "         [-3.0531],\n",
       "         [ 0.4773]])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "true_w = torch.tensor([2, -3.4])\n",
    "true_b = 4.2\n",
    "features, labels = synthetic_data(true_w, true_b, 1000)\n",
    "\n",
    "def load_array(data_arrays, batch_size, is_train=True):\n",
    "    dataset = data.TensorDataset(*data_arrays)\n",
    "    return data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
    "\n",
    "batch_size = 10\n",
    "data_iter = load_array((features, labels), batch_size)\n",
    "next(iter(data_iter))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
